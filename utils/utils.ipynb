{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96c78be",
   "metadata": {},
   "source": [
    "# Code crop da bo sung them skip in black list, doi voi truong hop cac phien hieu giua S2, S1 khong khop ve kich thuoc\n",
    "# Hien tai dang co 20 phien hieu khong khop kich thuoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import glob\n",
    "# import argparse\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "from rasterio.transform import from_bounds\n",
    "from tqdm import tqdm\n",
    "# import skimage.io\n",
    "import yaml\n",
    "import sys\n",
    "# __dir__ = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.append(__dir__)\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(__dir__, '../../')))\n",
    "\n",
    "# from ssr.utils.options import yaml_load\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "from rasterio.windows import Window\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_black_list(black_list_path) -> list:\n",
    "    black_list = []\n",
    "    try:\n",
    "        with open(black_list_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                black_list.append(os.path.basename(line.strip()).split('.')[0])\n",
    "        return black_list\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'{e}: Black list path isnt valid')\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def load_whitelist(whitelist_path) -> list:\n",
    "    \"\"\"Load whitelist of file names from a txt file.\n",
    "    \n",
    "    Args:\n",
    "        whitelist_path (str): Path to the whitelist file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of file names (without extension) from whitelist\n",
    "    \"\"\"\n",
    "    whitelist = []\n",
    "    try:\n",
    "        with open(whitelist_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                line = line.strip()\n",
    "                if line:  # Skip empty lines\n",
    "                    whitelist.append(os.path.basename(line).split('.')[0])\n",
    "        return whitelist\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'{e}: Whitelist path isnt valid')\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def in_black_list(filename ,black_list):\n",
    "    for fn in black_list:\n",
    "        if fn == filename:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def in_whitelist(filename, whitelist):\n",
    "    \"\"\"Check if filename is in whitelist.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): File name to check (without extension)\n",
    "        whitelist (list): List of whitelisted file names\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if filename is in whitelist, False otherwise\n",
    "    \"\"\"\n",
    "    for fn in whitelist:\n",
    "        if fn == filename:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def db2linear(image_data, v_min = -25, v_max = 5):\n",
    "    data_clipped = np.clip(image_data, v_min, v_max)\n",
    "    data_norm = (data_clipped-v_min)/(v_max-v_min)\n",
    "    data_uint8 = (data_norm * 255).astype(np.uint8)\n",
    "    return data_uint8\n",
    "\n",
    "\n",
    "def change_range_f(image_data, old_min, old_max, new_min, new_max):\n",
    "    new_image = ((image_data-old_min) * (new_max - new_min) / (old_max - old_min) + new_min).clip(new_min, new_max)\n",
    "    new_image = (new_image - new_min) / (new_max - new_min)\n",
    "    new_image = (new_image * 255).astype(np.uint8)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "\n",
    "def ordered_yaml():\n",
    "    \"\"\"Support OrderedDict for yaml.\n",
    "\n",
    "    Returns:\n",
    "        tuple: yaml Loader and Dumper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from yaml import CDumper as Dumper\n",
    "        from yaml import CLoader as Loader\n",
    "    except ImportError:\n",
    "        from yaml import Dumper, Loader\n",
    "\n",
    "    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n",
    "\n",
    "    def dict_representer(dumper, data):\n",
    "        return dumper.represent_dict(data.items())\n",
    "\n",
    "    def dict_constructor(loader, node):\n",
    "        return OrderedDict(loader.construct_pairs(node))\n",
    "\n",
    "    Dumper.add_representer(OrderedDict, dict_representer)\n",
    "    Loader.add_constructor(_mapping_tag, dict_constructor)\n",
    "    return Loader, Dumper\n",
    "\n",
    "\n",
    "def yaml_load(f):\n",
    "    \"\"\"Load yaml file or string.\n",
    "\n",
    "    Args:\n",
    "        f (str): File path or a python string.\n",
    "\n",
    "    Returns:\n",
    "        dict: Loaded dict.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, 'r') as f:\n",
    "            return yaml.load(f, Loader=ordered_yaml()[0])\n",
    "    else:\n",
    "        return yaml.load(f, Loader=ordered_yaml()[0])\n",
    "\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "num_cols = None\n",
    "num_rows = None\n",
    "height = None\n",
    "width = None\n",
    "\n",
    "\n",
    "def split_and_save_image_with_overlap_change(image_path, output_dir, \n",
    "                                             tile_size = 32,\n",
    "                                               split_factor: int = 2,\n",
    "                                                 remove_A:int = 1,\n",
    "                                              db2ln: bool = False,\n",
    "                                                sar_vv: bool = False,\n",
    "                                                  sar_vh: bool = False,\n",
    "                                                    change_range:bool = False,\n",
    "                                                downsample: int = 0,\n",
    "                                                  four_bands:bool = False,\n",
    "                                                  save_fmt: str = 'TIFF'):\n",
    "    old_min = -25\n",
    "    old_max = 0\n",
    "    new_min = -25\n",
    "    new_max = 5\n",
    "\n",
    "    def process_and_save(tile, row, col):\n",
    "        if db2ln: # convert [-25db, 0db] -> [0, 255] (Thuc te la 0-5000 -> 0-255 cho S2)\n",
    "            if sar_vv:\n",
    "                tile = db2linear(tile[:, :, 0])\n",
    "            elif sar_vh:\n",
    "                tile = db2linear(tile[:, :, 1])\n",
    "            else:\n",
    "                tile = db2linear(tile[:, :, :])\n",
    "        \n",
    "        if change_range:\n",
    "            if sar_vv:\n",
    "                tile = change_range_f(tile[:, :, 0], old_min = old_min, old_max = old_max, new_min = new_min, new_max = new_max)\n",
    "            elif sar_vh:\n",
    "                tile = change_range_f(tile[:, :, 1],  old_min = old_min, old_max = old_max, new_min = new_min, new_max = new_max)\n",
    "\n",
    "        tile_img = Image.fromarray(tile)\n",
    "\n",
    "        if downsample:\n",
    "            tile_img = tile_img.resize(size = (tile_size//downsample, tile_size//downsample), resample=Image.Resampling.BICUBIC)\n",
    "        \n",
    "        ext = '.png' if save_fmt.lower() == 'png' else '.tif'\n",
    "        tile_path = os.path.join(output_dir, f\"{row}_{col}_{os.path.splitext(os.path.basename(image_path))[0]}{ext}\")\n",
    "        tile_img.save(tile_path, format=save_fmt)\n",
    "\n",
    "    with rasterio.open(image_path) as src:\n",
    "        image = src.read()\n",
    "        transform = src.transform\n",
    "        width, height = src.width, src.height\n",
    "        array = np.array(image)\n",
    "\n",
    "        # print(f'Shape of {os.path.basename(image_path)}: {array.shape}') # HERE\n",
    "        if array.shape[1] < tile_size or array.shape[2] < tile_size:\n",
    "            print(f'Shape of image must not smaller than {tile_size}. NEXT!')\n",
    "            return\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    # print(image.shape) # debug\n",
    "    if remove_A:\n",
    "        image = image[:, :, 0:3]\n",
    "\n",
    "    # tile_size = 32\n",
    "    # print(height, tile_size, split_factor)\n",
    "    num_rows = height // int(tile_size  / split_factor)\n",
    "    num_cols = width // int(tile_size  / split_factor)\n",
    "    # print(\"num rows:, num col\",num_rows,num_cols)\n",
    "    # input()\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            start_row = row*(tile_size//split_factor)\n",
    "            end_row = row*(tile_size//split_factor) + tile_size\n",
    "            start_col = col*(tile_size//split_factor)\n",
    "            end_col = col*(tile_size//split_factor) + tile_size\n",
    "\n",
    "            if end_row <= height and end_col <= width:\n",
    "            # print(f'1: [{row*(tile_size//split_factor)}:{row*(tile_size//split_factor) + tile_size}, {col*(tile_size//split_factor)} : {(col)*(tile_size//split_factor) + tile_size}, :]')\n",
    "                tile = image[start_row: end_row, start_col: end_col, :]\n",
    "                process_and_save(tile, row, col)\n",
    "\n",
    "    # row = num_rows - 1 # HERE\n",
    "    # col = num_cols - 1 # HERE\n",
    "\n",
    "    if height % tile_size != 0:\n",
    "        # row = num_rows\n",
    "        for col in range(num_cols):\n",
    "            start_row = height - tile_size\n",
    "            start_col = col*(tile_size//split_factor)\n",
    "            end_col = col*(tile_size//split_factor) + tile_size\n",
    "\n",
    "            if end_col <= width:\n",
    "              \n",
    "                # print(f'2: [{start_row}:{height}, {col*(tile_size//split_factor)} : {(col)*(tile_size//split_factor) + tile_size}, :]')\n",
    "\n",
    "                tile = image[start_row:, start_col:end_col, :]\n",
    "                process_and_save(tile, row, col)\n",
    "\n",
    "    if width % tile_size != 0:\n",
    "        # col = num_cols\n",
    "        for row in range(num_rows):\n",
    "            start_col = width - tile_size\n",
    "            start_row = row * (tile_size // split_factor)\n",
    "            end_row = row * (tile_size // split_factor) + tile_size\n",
    "\n",
    "            # print(f'3: [{start_col}:{width}, {row*(tile_size//split_factor)} : {(row)*(tile_size//split_factor) + tile_size}, :]')\n",
    "            if end_row <= height:\n",
    "                \n",
    "                tile = image[start_row : end_row, start_col:, :]\n",
    "                process_and_save(tile, row, col)\n",
    "\n",
    "\n",
    "    if height % tile_size != 0 and width % tile_size != 0:\n",
    "        start_row = height - tile_size\n",
    "        start_col = width - tile_size\n",
    "        # row = num_rows\n",
    "        # col = num_cols\n",
    "        tile = image[start_row:, start_col:, :]\n",
    "        process_and_save(tile, row, col)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load configuration file\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('-opt', type=str, default= '/mnt/data1tb/vinh/ISSM-SAR/utils/config_crop_s2_lc_s1.yaml', help=\"Path to the options file.\")\n",
    "    # parser.add_argument('-remove_A', type=int, required=True)\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # opt = yaml_load(args.opt)\n",
    "    config_path = '/mnt/data1tb/vinh/ISSM-SAR/utils/config_crop_s2_lc_s1.yaml'\n",
    "    opt = yaml_load(config_path)\n",
    "    # number_of_random_img = opt['number_of_random_img'] # HERE\n",
    "    chip_size = opt['chip_size']\n",
    "    data_dir = opt['data_dir']\n",
    "    split_dir = opt['split_dir']\n",
    "    split_factor = opt['split_factor']\n",
    "    remove_A = opt['remove_A']\n",
    "    db2ln = opt['db2ln']\n",
    "    sar_vv = opt['sar_vv']\n",
    "    sar_vh = opt['sar_vh']\n",
    "    change_range = opt['change_range']\n",
    "    downsample = opt['downsample']\n",
    "    four_bands = opt['four_bands']\n",
    "    save_fmt = opt.get('save_fmt', 'TIFF')\n",
    "    black_list_path = opt['black_list_path']\n",
    "    whitelist_path = opt['white_list_path']  # Optional whitelist\n",
    "\n",
    "    black_list = load_black_list(black_list_path)\n",
    "    print(f'Loaded {len(black_list)} items in blacklist')\n",
    "    \n",
    "    whitelist = None\n",
    "    if whitelist_path and os.path.exists(whitelist_path):\n",
    "        whitelist = load_whitelist(whitelist_path)\n",
    "        print(f'Loaded {len(whitelist)} items in whitelist from {whitelist_path}')\n",
    "    else:\n",
    "        print('No whitelist provided - will process all files (except blacklisted)')\n",
    "    \n",
    "    count_in_black_list = 0\n",
    "    count_not_in_whitelist = 0\n",
    "    count_processed = 0\n",
    "    \n",
    "    # file_name_except = opt['file_name_except']\n",
    "    # List of tiff files in Dir S2 LR\n",
    "    tiff_files = [f for f in os.listdir(data_dir) if f.endswith('.tif')]\n",
    "    print(f'Found {len(tiff_files)} .tif files in {data_dir}')\n",
    "    \n",
    "    if remove_A:\n",
    "        print('Remove A channels of LandCover')\n",
    "    \n",
    "    for tiff_file in tqdm(tiff_files, desc=f\"Splitting to {chip_size}x{chip_size} tiles\"):\n",
    "        filename_without_ext = tiff_file.split('.')[0]\n",
    "        \n",
    "        # Check blacklist\n",
    "        if in_black_list(filename_without_ext, black_list):\n",
    "            print(f'SKIP - Blacklisted: {tiff_file}')\n",
    "            count_in_black_list += 1\n",
    "            continue\n",
    "        \n",
    "        # Check whitelist\n",
    "        if whitelist is not None:\n",
    "            if not in_whitelist(filename_without_ext, whitelist):\n",
    "                print(f'SKIP - Not in whitelist: {tiff_file}')\n",
    "                count_not_in_whitelist += 1\n",
    "                continue\n",
    "        \n",
    "        # print(\"Processing tile: \", tiff_file)\n",
    "        tiff_path = os.path.join(data_dir, tiff_file)\n",
    "        output_folder = os.path.join(split_dir)\n",
    "        ensure_dir(output_folder)\n",
    "\n",
    "        print(f'Processing: {tiff_file}')\n",
    "        split_and_save_image_with_overlap_change(tiff_path, output_folder, chip_size, \n",
    "                                                 split_factor, remove_A, \n",
    "                                                 db2ln, sar_vv, sar_vh, change_range, \n",
    "                                                 downsample, four_bands, save_fmt)\n",
    "        count_processed += 1\n",
    "    \n",
    "    print('='*50)\n",
    "    print('Processing Summary:')\n",
    "    print(f'Total files found: {len(tiff_files)}')\n",
    "    print(f'Files processed: {count_processed}')\n",
    "    print(f'Files in blacklist (skipped): {count_in_black_list}')\n",
    "    if whitelist is not None:\n",
    "        print(f'Files not in whitelist (skipped): {count_not_in_whitelist}')\n",
    "    print(f'Total output files: {len(os.listdir(split_dir))}')\n",
    "    print('='*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e06c76",
   "metadata": {},
   "source": [
    "# CROP SAR, SAVE DUOI DANG NPY [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c28112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module for cropping and saving raw SAR data as .npy files.\n",
    "- Clip values to [-25, 5] dB range\n",
    "- Normalize to [-1, 1] range\n",
    "- Save as .npy with filename format: row_col_originalfilename.npy\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def ordered_yaml():\n",
    "    \"\"\"Support OrderedDict for yaml.\n",
    "\n",
    "    Returns:\n",
    "        tuple: yaml Loader and Dumper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from yaml import CDumper as Dumper\n",
    "        from yaml import CLoader as Loader\n",
    "    except ImportError:\n",
    "        from yaml import Dumper, Loader\n",
    "\n",
    "    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n",
    "\n",
    "    def dict_representer(dumper, data):\n",
    "        return dumper.represent_dict(data.items())\n",
    "\n",
    "    def dict_constructor(loader, node):\n",
    "        return OrderedDict(loader.construct_pairs(node))\n",
    "\n",
    "    Dumper.add_representer(OrderedDict, dict_representer)\n",
    "    Loader.add_constructor(_mapping_tag, dict_constructor)\n",
    "    return Loader, Dumper\n",
    "\n",
    "\n",
    "def yaml_load(f):\n",
    "    \"\"\"Load yaml file or string.\"\"\"\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, 'r') as file:\n",
    "            return yaml.load(file, Loader=ordered_yaml()[0])\n",
    "    else:\n",
    "        return yaml.load(f, Loader=ordered_yaml()[0])\n",
    "\n",
    "\n",
    "def load_black_list(black_list_path) -> list:\n",
    "    \"\"\"Load blacklist of file names from a txt file.\"\"\"\n",
    "    black_list = []\n",
    "    try:\n",
    "        with open(black_list_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                black_list.append(os.path.basename(line.strip()).split('.')[0])\n",
    "        return black_list\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'{e}: Black list path isnt valid')\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_whitelist(whitelist_path) -> list:\n",
    "    \"\"\"Load whitelist of file names from a txt file.\"\"\"\n",
    "    whitelist = []\n",
    "    try:\n",
    "        with open(whitelist_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    whitelist.append(os.path.basename(line).split('.')[0])\n",
    "        return whitelist\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'{e}: Whitelist path isnt valid')\n",
    "        return []\n",
    "\n",
    "\n",
    "def in_black_list(filename, black_list):\n",
    "    return filename in black_list\n",
    "\n",
    "\n",
    "def in_whitelist(filename, whitelist):\n",
    "    return filename in whitelist\n",
    "\n",
    "\n",
    "def clip_and_normalize_to_minus1_1(data, v_min=-25, v_max=5):\n",
    "    \"\"\"\n",
    "    Clip data to [v_min, v_max] then normalize to [-1, 1] range.\n",
    "    \n",
    "    Args:\n",
    "        data: Input numpy array (SAR data in dB)\n",
    "        v_min: Minimum clip value (default: -25 dB)\n",
    "        v_max: Maximum clip value (default: 5 dB)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized numpy array in range [-1, 1] as float32\n",
    "    \"\"\"\n",
    "    data_clipped = np.clip(data, v_min, v_max)\n",
    "    # Normalize to [0, 1] first\n",
    "    data_norm = (data_clipped - v_min) / (v_max - v_min)\n",
    "    # Convert to [-1, 1]\n",
    "    data_normalized = data_norm * 2 - 1\n",
    "    return data_normalized.astype(np.float32)\n",
    "\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def split_and_save_raw_npy(image_path, output_dir, \n",
    "                           tile_size=128,\n",
    "                           split_factor=4,\n",
    "                           remove_A=False,\n",
    "                           sar_vv=True,\n",
    "                           sar_vh=False,\n",
    "                           v_min=-25,\n",
    "                           v_max=5,\n",
    "                           save_fmt='npy'):\n",
    "    \"\"\"\n",
    "    Split image into tiles, clip to [v_min, v_max], normalize to [-1, 1], \n",
    "    and save as .npy or .npz files.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input TIFF file\n",
    "        output_dir: Output directory for saved files\n",
    "        tile_size: Size of each tile (default: 128)\n",
    "        split_factor: Overlap factor for splitting (default: 4)\n",
    "        remove_A: Whether to remove alpha channel (default: False)\n",
    "        sar_vv: Use only VV band (default: True)\n",
    "        sar_vh: Use only VH band (default: False)\n",
    "        v_min: Minimum clip value (default: -25 dB)\n",
    "        v_max: Maximum clip value (default: 5 dB)\n",
    "        save_fmt: Save format - 'npy' or 'npz' (default: 'npy')\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_and_save_raw(tile, row, col):\n",
    "        # Clip and normalize to [-1, 1]\n",
    "        tile_normalized = clip_and_normalize_to_minus1_1(tile, v_min, v_max)\n",
    "        \n",
    "        # Generate filename: row_col_originalfilename.npy\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        ext = '.npy' if save_fmt.lower() == 'npy' else '.npz'\n",
    "        tile_path = os.path.join(output_dir, f\"{row}_{col}_{base_name}{ext}\")\n",
    "        \n",
    "        if save_fmt.lower() == 'npy':\n",
    "            np.save(tile_path, tile_normalized)\n",
    "        else:\n",
    "            np.savez_compressed(tile_path, data=tile_normalized)\n",
    "\n",
    "    with rasterio.open(image_path) as src:\n",
    "        image = src.read()\n",
    "        width, height = src.width, src.height\n",
    "        array = np.array(image)\n",
    "\n",
    "        if array.shape[1] < tile_size or array.shape[2] < tile_size:\n",
    "            print(f'Shape of image must not smaller than {tile_size}. NEXT!')\n",
    "            return 0\n",
    "    \n",
    "    image = np.transpose(image, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "    \n",
    "    if remove_A and image.shape[2] > 3:\n",
    "        image = image[:, :, 0:3]\n",
    "\n",
    "    # Select channel based on sar_vv or sar_vh\n",
    "    if sar_vv:\n",
    "        image = image[:, :, 0:1]  # VV only, keep dims\n",
    "    elif sar_vh:\n",
    "        image = image[:, :, 1:2]  # VH only, keep dims\n",
    "\n",
    "    num_rows = height // int(tile_size / split_factor)\n",
    "    num_cols = width // int(tile_size / split_factor)\n",
    "    \n",
    "    tile_count = 0\n",
    "    \n",
    "    # Process main grid\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            start_row = row * (tile_size // split_factor)\n",
    "            end_row = row * (tile_size // split_factor) + tile_size\n",
    "            start_col = col * (tile_size // split_factor)\n",
    "            end_col = col * (tile_size // split_factor) + tile_size\n",
    "\n",
    "            if end_row <= height and end_col <= width:\n",
    "                tile = image[start_row:end_row, start_col:end_col, :]\n",
    "                process_and_save_raw(tile, row, col)\n",
    "                tile_count += 1\n",
    "\n",
    "    # Handle remaining rows (bottom edge)\n",
    "    if height % tile_size != 0:\n",
    "        for col in range(num_cols):\n",
    "            start_row = height - tile_size\n",
    "            start_col = col * (tile_size // split_factor)\n",
    "            end_col = col * (tile_size // split_factor) + tile_size\n",
    "\n",
    "            if end_col <= width:\n",
    "                tile = image[start_row:, start_col:end_col, :]\n",
    "                process_and_save_raw(tile, row, col)\n",
    "                tile_count += 1\n",
    "\n",
    "    # Handle remaining columns (right edge)\n",
    "    if width % tile_size != 0:\n",
    "        for row in range(num_rows):\n",
    "            start_col = width - tile_size\n",
    "            start_row = row * (tile_size // split_factor)\n",
    "            end_row = row * (tile_size // split_factor) + tile_size\n",
    "\n",
    "            if end_row <= height:\n",
    "                tile = image[start_row:end_row, start_col:, :]\n",
    "                process_and_save_raw(tile, row, col)\n",
    "                tile_count += 1\n",
    "\n",
    "    # Handle corner (bottom-right)\n",
    "    if height % tile_size != 0 and width % tile_size != 0:\n",
    "        start_row = height - tile_size\n",
    "        start_col = width - tile_size\n",
    "        tile = image[start_row:, start_col:, :]\n",
    "        process_and_save_raw(tile, row, col)\n",
    "        tile_count += 1\n",
    "    \n",
    "    return tile_count\n",
    "\n",
    "\n",
    "def main(config_path):\n",
    "    \"\"\"\n",
    "    Main function to process all TIFF files based on config.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to YAML config file\n",
    "    \"\"\"\n",
    "    opt = yaml_load(config_path)\n",
    "    \n",
    "    chip_size = opt.get('chip_size', 128)\n",
    "    data_dir = opt['data_dir']\n",
    "    split_dir = opt.get('split_dir_raw')  # Default: add _raw suffix\n",
    "    split_factor = opt.get('split_factor', 4)\n",
    "    remove_A = opt.get('remove_A', False)\n",
    "    sar_vv = opt.get('sar_vv', True)\n",
    "    sar_vh = opt.get('sar_vh', False)\n",
    "    v_min = opt.get('v_min', -25)\n",
    "    v_max = opt.get('v_max', 5)\n",
    "    save_fmt = opt.get('raw_save_fmt', 'npy')\n",
    "    black_list_path = opt.get('black_list_path', '')\n",
    "    whitelist_path = opt.get('white_list_path', '')\n",
    "\n",
    "    # Load blacklist and whitelist\n",
    "    black_list = load_black_list(black_list_path) if black_list_path else []\n",
    "    if black_list:\n",
    "        print(f'Loaded {len(black_list)} items in blacklist')\n",
    "    \n",
    "    whitelist = None\n",
    "    if whitelist_path and os.path.exists(whitelist_path):\n",
    "        whitelist = load_whitelist(whitelist_path)\n",
    "        print(f'Loaded {len(whitelist)} items in whitelist from {whitelist_path}')\n",
    "    else:\n",
    "        print('No whitelist provided - will process all files (except blacklisted)')\n",
    "    \n",
    "    count_in_black_list = 0\n",
    "    count_not_in_whitelist = 0\n",
    "    count_processed = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    # List TIFF files\n",
    "    tiff_files = [f for f in os.listdir(data_dir) if f.endswith('.tif')]\n",
    "    print(f'Found {len(tiff_files)} .tif files in {data_dir}')\n",
    "    \n",
    "    ensure_dir(split_dir)\n",
    "    \n",
    "    for tiff_file in tqdm(tiff_files, desc=f\"Splitting to {chip_size}x{chip_size} raw tiles\"):\n",
    "        filename_without_ext = tiff_file.split('.')[0]\n",
    "        \n",
    "        # Check blacklist\n",
    "        if black_list and in_black_list(filename_without_ext, black_list):\n",
    "            print(f'SKIP - Blacklisted: {tiff_file}')\n",
    "            count_in_black_list += 1\n",
    "            continue\n",
    "        \n",
    "        # Check whitelist\n",
    "        if whitelist is not None:\n",
    "            if not in_whitelist(filename_without_ext, whitelist):\n",
    "                print(f'SKIP - Not in whitelist: {tiff_file}')\n",
    "                count_not_in_whitelist += 1\n",
    "                continue\n",
    "        \n",
    "        tiff_path = os.path.join(data_dir, tiff_file)\n",
    "        \n",
    "        print(f'Processing: {tiff_file}')\n",
    "        tiles = split_and_save_raw_npy(\n",
    "            tiff_path, split_dir, chip_size, split_factor,\n",
    "            remove_A, sar_vv, sar_vh, v_min, v_max, save_fmt\n",
    "        )\n",
    "        total_tiles += tiles\n",
    "        count_processed += 1\n",
    "    \n",
    "    print('=' * 50)\n",
    "    print('Processing Summary:')\n",
    "    print(f'Total files found: {len(tiff_files)}')\n",
    "    print(f'Files processed: {count_processed}')\n",
    "    print(f'Files in blacklist (skipped): {count_in_black_list}')\n",
    "    if whitelist is not None:\n",
    "        print(f'Files not in whitelist (skipped): {count_not_in_whitelist}')\n",
    "    print(f'Total output tiles: {total_tiles}')\n",
    "    print(f'Output directory: {split_dir}')\n",
    "    print('=' * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # import argparse\n",
    "    config_path = r'/mnt/data1tb/vinh/ISSM-SAR/utils/config_crop_raw.yaml'\n",
    "    # parser = argparse.ArgumentParser(description='Crop and save raw SAR data as .npy files')\n",
    "    # parser.add_argument('-opt', '--config', type=str, \n",
    "    #                     default='/mnt/data1tb/vinh/ISSM-SAR/utils/config_crop_raw.yaml',\n",
    "    #                     help='Path to the config file')\n",
    "    # args = parser.parse_args()\n",
    "    # config = yaml_load(config_path)\n",
    "    main(config_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9389d",
   "metadata": {},
   "source": [
    "# Check NAN or Inf cua folder chua .npy\n",
    "Xoa dong thoi o 3 folder S1T1, S1T2, S1HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc62222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67086 .npy files in /mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune/S1T2\n",
      "\n",
      "\n",
      "========================================\n",
      "Summary:\n",
      "Checked: 67086\n",
      "Marked for deletion: 16\n",
      "Moved: 48, Removed: 0, Missing counterpart files: 0\n",
      "========================================\n",
      "\n",
      "Examples (first 10):\n",
      " - 4_2_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 3_5_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 4_3_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 3_3_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 5_3_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 4_5_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 5_4_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 5_2_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 2_3_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n",
      " - 5_5_D_49_51_A_d_3.npy: NaN_count=1, NaN_ratio=0.01%\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ========== CẤU HÌNH ==========\n",
    "BASE_DIR = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune\"\n",
    "CHECK_DIR = os.path.join(BASE_DIR, \"S1T2\")   # folder để quét file .npy (có thể là S1T1/S1HR tùy bạn)\n",
    "FOLDERS_TO_CLEAN = [os.path.join(BASE_DIR, p) for p in [\"S1T1\", \"S1T2\", \"S1HR\"]]\n",
    "\n",
    "nan_ratio_threshold = 0.0   # ngưỡng % NaN (> threshold -> xóa)\n",
    "delete_if_any_nan = True   # nếu True: xóa mọi file có ít nhất 1 NaN\n",
    "dry_run = False              # True = chỉ in log, False = thực hiện xóa\n",
    "delete_method = \"move\"      # \"move\" hoặc \"remove\"\n",
    "TRASH_DIR = os.path.join(BASE_DIR, \"deleted_nan\")  # nếu move -> lưu ở đây\n",
    "# ==============================\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "npy_files = list(Path(CHECK_DIR).glob(\"*.npy\"))\n",
    "print(f\"Found {len(npy_files)} .npy files in {CHECK_DIR}\\n\")\n",
    "\n",
    "counts = {\"checked\":0, \"to_delete\":0, \"deleted\":0, \"moved\":0, \"missing\":0}\n",
    "problem_files = []\n",
    "\n",
    "ensure_dir(TRASH_DIR)\n",
    "\n",
    "for p in npy_files:\n",
    "    fname = p.name\n",
    "    arr = np.load(p)\n",
    "    total = arr.size\n",
    "    nan_count = np.isnan(arr).sum()\n",
    "    nan_ratio = nan_count / total if total>0 else 0.0\n",
    "    has_nan = nan_count > 0\n",
    "\n",
    "    counts[\"checked\"] += 1\n",
    "\n",
    "    should_delete = False\n",
    "    if delete_if_any_nan and has_nan:\n",
    "        should_delete = True\n",
    "    if nan_ratio > nan_ratio_threshold:\n",
    "        should_delete = True\n",
    "\n",
    "    if should_delete:\n",
    "        counts[\"to_delete\"] += 1\n",
    "        problem_files.append((fname, nan_count, nan_ratio))\n",
    "        for folder in FOLDERS_TO_CLEAN:\n",
    "            src = os.path.join(folder, fname)\n",
    "            if os.path.exists(src):\n",
    "                if dry_run:\n",
    "                    print(f\"[DRY] Would {delete_method} {src} (NaN ratio={nan_ratio:.2%})\")\n",
    "                else:\n",
    "                    if delete_method == \"move\":\n",
    "                        dest_dir = os.path.join(TRASH_DIR, os.path.basename(folder))\n",
    "                        ensure_dir(dest_dir)\n",
    "                        shutil.move(src, os.path.join(dest_dir, fname))\n",
    "                        counts[\"moved\"] += 1\n",
    "                    else: # remove\n",
    "                        os.remove(src)\n",
    "                        counts[\"deleted\"] += 1\n",
    "            else:\n",
    "                counts[\"missing\"] += 1\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Summary:\")\n",
    "print(f\"Checked: {counts['checked']}\")\n",
    "print(f\"Marked for deletion: {counts['to_delete']}\")\n",
    "print(f\"Moved: {counts['moved']}, Removed: {counts['deleted']}, Missing counterpart files: {counts['missing']}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Optional: print first problem entries\n",
    "if problem_files:\n",
    "    print(\"\\nExamples (first 10):\")\n",
    "    for fname, ncount, nratio in problem_files[:10]:\n",
    "        print(f\" - {fname}: NaN_count={ncount}, NaN_ratio={nratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24837054",
   "metadata": {},
   "source": [
    "# Ham visualize triplet (input s1t1, s1t2) va target S1SR\n",
    "S1T1, S1T2: Folder path <br>\n",
    "S1SR: Folder path/s1_gen.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3230ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_yaml_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            return config\n",
    "    except:\n",
    "        print('Config path is not valid')\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def visualize_triplet_from_config(config_path):\n",
    "    \"\"\"\n",
    "    Load configuration from YAML file and visualize triplet images.\n",
    "    \n",
    "    Args:\n",
    "        config_path (str): Path to plot_config.yaml file\n",
    "    \"\"\"\n",
    "    config = load_yaml_config(config_path)\n",
    "    visualize_triplet(\n",
    "        s1t1_dir=config['s1t1_dir'],\n",
    "        s1t2_dir=config['s1t2_dir'],\n",
    "        s1sr_dir=config['s1sr_dir'],\n",
    "        output_dir=config['output_dir'],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "def visualize_triplet(s1t1_dir, s1t2_dir, s1sr_dir, output_dir, config=None):\n",
    "    \"\"\"\n",
    "    Visualize triplet images (S1T1, S1T2, S1SR) horizontally.\n",
    "    \n",
    "    Structure:\n",
    "    - s1t1_dir: Contains images like abc.png, def.png, etc.\n",
    "    - s1t2_dir: Contains images with same names as s1t1\n",
    "    - s1sr_dir: Contains subfolders with names matching s1t1/s1t2 files (without extension).\n",
    "               Each subfolder contains s1_gen.png\n",
    "    \n",
    "    Args:\n",
    "        s1t1_dir (str): Path to S1T1 folder containing images.\n",
    "        s1t2_dir (str): Path to S1T2 folder containing images with same names as S1T1.\n",
    "        s1sr_dir (str): Path to S1SR folder containing subfolders with s1_gen.png files.\n",
    "        output_dir (str): Path to save the plots.\n",
    "        config (dict): Configuration dictionary with advanced options\n",
    "    \"\"\"\n",
    "    # Default config values\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    \n",
    "    max_images = config.get('max_images', None)\n",
    "    filename_pattern = config.get('filename_pattern', None)\n",
    "    skip_pattern = config.get('skip_pattern', None)\n",
    "    start_index = config.get('start_index', 0)\n",
    "    figure_size = config.get('figure_size', {'width': 12, 'height': 4})\n",
    "    colormap = config.get('image_colormap', 'gray')\n",
    "    interpolation = config.get('interpolation', 'nearest')\n",
    "    show_title = config.get('show_title', True)\n",
    "    title_fontsize = config.get('title_fontsize', 12)\n",
    "    show_axis = config.get('show_axis', False)\n",
    "    wspace = config.get('wspace', 0.05)\n",
    "    hspace = config.get('hspace', 0.3)\n",
    "    save_format = config.get('save_format', 'png')\n",
    "    save_dpi = config.get('save_dpi', 150)\n",
    "    bbox_inches = config.get('bbox_inches', 'tight')\n",
    "    pad_inches = config.get('pad_inches', 0.05)\n",
    "    verbose = config.get('verbose', True)\n",
    "    report_missing = config.get('report_missing', True)\n",
    "    skip_incomplete = config.get('skip_incomplete', True)\n",
    "    continue_on_error = config.get('continue_on_error', True)\n",
    "    image_extensions = config.get('image_extensions', ['*.png', '*.jpg', '*.tif', '*.tiff'])\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get list of images from S1T1\n",
    "    files = []\n",
    "    for ext in image_extensions:\n",
    "        files.extend(glob.glob(os.path.join(s1t1_dir, ext)))\n",
    "    \n",
    "    files.sort()\n",
    "    \n",
    "    # Apply filename pattern filter\n",
    "    if filename_pattern:\n",
    "        pattern = re.compile(filename_pattern)\n",
    "        files = [f for f in files if pattern.search(os.path.basename(f))]\n",
    "    \n",
    "    # Apply skip pattern filter\n",
    "    if skip_pattern:\n",
    "        skip_re = re.compile(skip_pattern)\n",
    "        files = [f for f in files if not skip_re.search(os.path.basename(f))]\n",
    "    \n",
    "    # Apply start index\n",
    "    files = files[start_index:]\n",
    "    \n",
    "    # Apply max images limit\n",
    "    if max_images:\n",
    "        files = files[:max_images]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(files)} images in {s1t1_dir}\")\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    missing_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for idx, s1t1_path in enumerate(tqdm(files, desc=\"Visualizing\"), start=start_index):\n",
    "        filename = os.path.basename(s1t1_path)\n",
    "        filename_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Construct paths for S1T2 and S1SR\n",
    "        s1t2_path = os.path.join(s1t2_dir, filename)\n",
    "        s1sr_path = os.path.join(s1sr_dir, filename_without_ext, 's1_gen.png')\n",
    "        \n",
    "        if os.path.exists(s1t2_path) and os.path.exists(s1sr_path):\n",
    "            try:\n",
    "                img1 = Image.open(s1t1_path).convert('L')\n",
    "                img2 = Image.open(s1t2_path).convert('L')\n",
    "                img3 = Image.open(s1sr_path).convert('L')\n",
    "                \n",
    "                # Plot 1 row, 3 columns\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(figure_size['width'], figure_size['height']))\n",
    "                \n",
    "                # Display images\n",
    "                for ax, img, title in zip(axes, [img1, img2, img3], ['S1T1', 'S1T2', 'S1SR']):\n",
    "                    ax.imshow(img, cmap=colormap, interpolation=interpolation)\n",
    "                    ax.set_title(title, fontsize=title_fontsize)\n",
    "                    if not show_axis:\n",
    "                        ax.axis('off')\n",
    "                \n",
    "                # Set main title as filename\n",
    "                if show_title:\n",
    "                    fig.suptitle(filename, fontsize=title_fontsize)\n",
    "                \n",
    "                # Adjust layout: tight with small spacing\n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "                \n",
    "                # Save figure\n",
    "                ext = f'.{save_format}' if not save_format.startswith('.') else save_format\n",
    "                save_path = os.path.join(output_dir, filename_without_ext + ext)\n",
    "                plt.savefig(save_path, bbox_inches=bbox_inches, pad_inches=pad_inches, dpi=save_dpi)\n",
    "                plt.close(fig)\n",
    "                success_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                if verbose:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                if not continue_on_error:\n",
    "                    raise\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            missing_count += 1\n",
    "            if report_missing:\n",
    "                if not os.path.exists(s1t2_path):\n",
    "                    if verbose:\n",
    "                        print(f\"Missing S1T2: {s1t2_path}\")\n",
    "                if not os.path.exists(s1sr_path):\n",
    "                    if verbose:\n",
    "                        print(f\"Missing S1SR: {s1sr_path}\")\n",
    "            if skip_incomplete:\n",
    "                continue\n",
    "    \n",
    "    # Print summary\n",
    "    print('='*60)\n",
    "    print('Processing Summary:')\n",
    "    print(f'Total files found: {len(files)}')\n",
    "    print(f'Successfully processed: {success_count}')\n",
    "    print(f'Errors: {error_count}')\n",
    "    print(f'Skipped (missing files): {missing_count}')\n",
    "    print(f'Total output files: {len(os.listdir(output_dir))}')\n",
    "    print('='*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Load from config file\n",
    "    config_path = '/mnt/data1tb/vinh/ISSM-SAR/utils/plot_config.yaml'\n",
    "    visualize_triplet_from_config(config_path)\n",
    "    \n",
    "    # Option 2: Manual call\n",
    "    # s1t1_dir = \"/mnt/data1tb/vinh/data_tumlum/s1t1_0_7_days_cutted\"\n",
    "    # s1t2_dir = \"/mnt/data1tb/vinh/data_tumlum/s1t2_-7_0_days_cutted\"\n",
    "    # s1sr_dir = \"\"\n",
    "    # output_dir = \"\"\n",
    "    # visualize_triplet(s1t1_dir, s1t2_dir, s1sr_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e712e9",
   "metadata": {},
   "source": [
    "# Split train/valid FINE-TUNE SET FOR ISSM SRSAR\n",
    "Đối với các tên mảnh C gốc, thì 80% số mảnh C gốc sẽ về train, với mỗi mảnh C gốc sẽ là toàn bộ các bản cắt. Còn lại về valid. Tương tự đối với tên mảnh D, E, F gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1094163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 632, in get\n",
      "    value = obj._trait_values[self.name]\n",
      "KeyError: '_control_lock'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 301, in dispatch_control\n",
      "    async with self._control_lock:\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 687, in __get__\n",
      "    return t.cast(G, self.get(obj, cls))  # the G should encode the Optional\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 649, in get\n",
      "    value = self._validate(obj, default)\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 722, in _validate\n",
      "    value = self.validate(obj, value)\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 2311, in validate\n",
      "    self.error(obj, value)\n",
      "  File \"/mnt/data1tb/conda_envs/issm/lib/python3.9/site-packages/traitlets/traitlets.py\", line 831, in error\n",
      "    raise TraitError(e)\n",
      "traitlets.traitlets.TraitError: The '_control_lock' trait of an IPythonKernel instance expected a Lock, not the NoneType None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Đang quét dữ liệu từ folder S1T1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phân tích file: 100%|██████████| 67847/67847 [00:00<00:00, 970456.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Thống kê và Phân chia (80/20):\n",
      "Category   | Total Scenes | Train Scenes | Val Scenes  \n",
      "-------------------------------------------------------\n",
      "C          | 86           | 68           | 18          \n",
      "D          | 22           | 17           | 5           \n",
      "E          | 31           | 24           | 7           \n",
      "F          | 38           | 30           | 8           \n",
      "\n",
      ">>> Bắt đầu di chuyển file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving to TRAIN: 100%|██████████| 139/139 [1:27:34<00:00, 37.80s/it]\n",
      "Moving to VAL: 100%|██████████| 38/38 [27:36<00:00, 43.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "HOÀN TẤT!\n",
      "Tổng số Scenes Train: 139\n",
      "Tổng số Patches Train: 53236\n",
      "------------------------------\n",
      "Tổng số Scenes Val:   38\n",
      "Tổng số Patches Val:   14611\n",
      "Dữ liệu lưu tại: /mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune_splited\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# ================= CẤU HÌNH (CONFIG) =================\n",
    "# Đường dẫn folder chứa các patch .npy hiện tại\n",
    "# Cấu trúc giả định: source_dir/S1T1, source_dir/S1T2, ...\n",
    "SOURCE_DIR = '/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune' \n",
    "\n",
    "# Đường dẫn folder đích để lưu dataset chuẩn\n",
    "OUTPUT_DIR = '/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune_splited'\n",
    "\n",
    "# Tỷ lệ Train (0.8 = 80%)\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Seed cố định để kết quả giống nhau mỗi lần chạy\n",
    "RANDOM_SEED = 42\n",
    "# =====================================================\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "def parse_filename(patch_filename):\n",
    "    \"\"\"\n",
    "    Input: '0_0_F_48_93_B_d_4.npy'\n",
    "    Output: \n",
    "        - scene_id: 'F_48_93_B_d_4' (Tên ảnh gốc)\n",
    "        - category: 'F' (Chữ cái đầu tiên để phân loại)\n",
    "    \"\"\"\n",
    "    # Loại bỏ đuôi mở rộng\n",
    "    name_no_ext = os.path.splitext(patch_filename)[0]\n",
    "    \n",
    "    # Tách chuỗi. Format: row_col_SceneName\n",
    "    parts = name_no_ext.split('_')\n",
    "    \n",
    "    # Scene name bắt đầu từ phần tử thứ 2 trở đi (bỏ row, col)\n",
    "    # VD: parts = ['0', '0', 'F', '48', '93', 'B', 'd', '4']\n",
    "    scene_id = \"_\".join(parts[2:]) \n",
    "    \n",
    "    # Category là phần tử đầu tiên của scene name (VD: 'F')\n",
    "    category = parts[2] \n",
    "    \n",
    "    return scene_id, category\n",
    "\n",
    "def main():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    sub_folders = ['S1T1', 'S1T2', 'S1HR']\n",
    "    \n",
    "    # Kiểm tra đường dẫn\n",
    "    for sub in sub_folders:\n",
    "        if not os.path.exists(os.path.join(SOURCE_DIR, sub)):\n",
    "            print(f\"Lỗi: Không tìm thấy folder {sub} trong {SOURCE_DIR}\")\n",
    "            return\n",
    "\n",
    "    print(\">>> Đang quét dữ liệu từ folder S1T1...\")\n",
    "    # Lấy list file từ S1T1 làm chuẩn\n",
    "    s1t1_files = [f for f in os.listdir(os.path.join(SOURCE_DIR, 'S1T1')) if f.endswith('.npy')]\n",
    "    \n",
    "    # 1. GROUPING: Gom nhóm patch theo Scene và Category\n",
    "    # Cấu trúc: category_dict = { 'C': ['Scene1', 'Scene2'], 'D': ['Scene3'] ... }\n",
    "    # Cấu trúc: scene_to_patches = { 'Scene1': ['0_0_Scene1.npy', '0_1_Scene1.npy'] }\n",
    "    category_to_scenes = defaultdict(list)\n",
    "    scene_to_patches = defaultdict(list)\n",
    "    \n",
    "    # Set để theo dõi scene đã add chưa để tránh duplicate trong list category\n",
    "    seen_scenes = set()\n",
    "\n",
    "    for f in tqdm(s1t1_files, desc=\"Phân tích file\"):\n",
    "        scene_id, category = parse_filename(f)\n",
    "        \n",
    "        scene_to_patches[scene_id].append(f)\n",
    "        \n",
    "        if scene_id not in seen_scenes:\n",
    "            category_to_scenes[category].append(scene_id)\n",
    "            seen_scenes.add(scene_id)\n",
    "            \n",
    "    # 2. STRATIFIED SPLIT: Chia theo từng Category\n",
    "    train_scenes_final = []\n",
    "    val_scenes_final = []\n",
    "    \n",
    "    print(\"\\n>>> Thống kê và Phân chia (80/20):\")\n",
    "    print(f\"{'Category':<10} | {'Total Scenes':<12} | {'Train Scenes':<12} | {'Val Scenes':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    sorted_categories = sorted(category_to_scenes.keys())\n",
    "    \n",
    "    for cat in sorted_categories:\n",
    "        scenes = category_to_scenes[cat]\n",
    "        # Xáo trộn ngẫu nhiên danh sách các ảnh gốc trong category này\n",
    "        random.shuffle(scenes)\n",
    "        \n",
    "        # Tính điểm cắt\n",
    "        n_total = len(scenes)\n",
    "        n_train = int(n_total * TRAIN_RATIO)\n",
    "        \n",
    "        # Chia\n",
    "        train_subset = scenes[:n_train]\n",
    "        val_subset = scenes[n_train:]\n",
    "        \n",
    "        # Cộng dồn vào danh sách tổng\n",
    "        train_scenes_final.extend(train_subset)\n",
    "        val_scenes_final.extend(val_subset)\n",
    "        \n",
    "        print(f\"{cat:<10} | {n_total:<12} | {len(train_subset):<12} | {len(val_subset):<12}\")\n",
    "\n",
    "    # 3. MOVING FILES: Di chuyển file thực tế\n",
    "    print(\"\\n>>> Bắt đầu di chuyển file...\")\n",
    "    \n",
    "    # Tạo folder đích\n",
    "    for phase in ['train', 'val']:\n",
    "        for sub in sub_folders:\n",
    "            ensure_dir(os.path.join(OUTPUT_DIR, phase, sub))\n",
    "\n",
    "    def move_data(scene_list, phase):\n",
    "        count_files = 0\n",
    "        for scene in tqdm(scene_list, desc=f\"Moving to {phase.upper()}\"):\n",
    "            patches = scene_to_patches[scene]\n",
    "            for patch_name in patches:\n",
    "                # Di chuyển đồng bộ cả 3 folder\n",
    "                for sub in sub_folders:\n",
    "                    src = os.path.join(SOURCE_DIR, sub, patch_name)\n",
    "                    dst = os.path.join(OUTPUT_DIR, phase, sub, patch_name)\n",
    "                    \n",
    "                    if os.path.exists(src):\n",
    "                        shutil.copy2(src, dst) # Dùng copy để an toàn (hoặc shutil.move để cắt)\n",
    "                    else:\n",
    "                        print(f\"Warning: Thiếu file {src}\")\n",
    "                count_files += 1\n",
    "        return count_files\n",
    "\n",
    "    n_train_files = move_data(train_scenes_final, 'train')\n",
    "    n_val_files = move_data(val_scenes_final, 'val')\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"HOÀN TẤT!\")\n",
    "    print(f\"Tổng số Scenes Train: {len(train_scenes_final)}\")\n",
    "    print(f\"Tổng số Patches Train: {n_train_files}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Tổng số Scenes Val:   {len(val_scenes_final)}\")\n",
    "    print(f\"Tổng số Patches Val:   {n_val_files}\")\n",
    "    print(f\"Dữ liệu lưu tại: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d359a1",
   "metadata": {},
   "source": [
    "# !!!! NGHI KY TRUOC KHI SU DUNG !!!!\n",
    "# CO XOA FILE\n",
    "# Check thieu du lieu o S1HR goc so voi S1T1 vaf S1T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb67613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "s1t1_folder_path = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune/s1t1\"\n",
    "s1t2_folder_path = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune/s1t2\"\n",
    "s1hr_folder_path = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune/s1hr\"\n",
    "\n",
    "count_s1t2 = 0\n",
    "count_s1hr = 0\n",
    "\n",
    "for fn in os.listdir(s1t1_folder_path):\n",
    "    s1t1_fp = os.path.join(s1t1_folder_path, fn)\n",
    "    s1t2_fp = os.path.join(s1t2_folder_path, fn)\n",
    "    s1hr_fp = os.path.join(s1hr_folder_path, fn)\n",
    "\n",
    "    if not os.path.exists(s1t2_fp):\n",
    "        count_s1t2 += 1\n",
    "        print(f'Missing S1T2: {fn}')\n",
    "\n",
    "    if not os.path.exists(s1hr_fp):\n",
    "        count_s1hr += 1\n",
    "        print(f'Missing S1HR: {fn}')\n",
    "        os.remove(s1t1_fp) # !!!!!!!!!!!!!!!!!!\n",
    "        os.remove(s1t2_fp) # !!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "print(f'{30*\"=\"}')\n",
    "print(f'Missing S1T2: {count_s1t2}')\n",
    "print(f'Missing S1HR: {count_s1hr}')\n",
    "print(f'The number of S1T1: {len(os.listdir(s1t1_folder_path))}')\n",
    "print(f'The number of S1T2: {len(os.listdir(s1t2_folder_path))}')\n",
    "print(f'The number of S1HR: {len(os.listdir(s1hr_folder_path))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16672a",
   "metadata": {},
   "source": [
    "# Visualize S1T1, S1T2, S1SR, S1HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335c57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quadruplets: S1T1 (.npy), S1T2 (.npy), S1SR (image), S1HR (.npy)\n",
    "# - Detect channel-first (C,H,W) and channel-last (H,W,C) .npy and fix\n",
    "# - Adds histograms for S1SR and S1HR distributions\n",
    "# - Tightens figure layout to remove extra whitespace\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_matching_npy(basename, folder):\n",
    "    \"\"\"Return first .npy file in folder that contains basename in filename, or None.\"\"\"\n",
    "    if not folder or not os.path.exists(folder):\n",
    "        return None\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith('.npy') and basename in f:\n",
    "            return os.path.join(folder, f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_hw(arr):\n",
    "    \"\"\"Return a 2D array (H,W) suitable for display from a .npy array.\n",
    "    Handles shapes: (H,W), (H,W,1), (H,W,C), (C,H,W).\n",
    "    Selects the first channel when multiple channels exist.\n",
    "    Maps values in [-1,1] -> [0,1], otherwise rescales by min/max as fallback.\"\"\"\n",
    "    if arr.ndim == 3:\n",
    "        # Detect channel-first (C,H,W) vs channel-last (H,W,C)\n",
    "        c0, c1, c2 = arr.shape\n",
    "        # If first dim is small (likely channels) and others larger -> channel-first\n",
    "        if c0 <= 4 and (c1 > c0 or c2 > c0):\n",
    "            arr = np.transpose(arr, (1, 2, 0))  # -> (H, W, C)\n",
    "\n",
    "        # Now arr is channel-last if still 3D\n",
    "        if arr.ndim == 3:\n",
    "            if arr.shape[2] == 1:\n",
    "                arr = arr[:, :, 0]\n",
    "            elif arr.shape[2] > 1:\n",
    "                arr = arr[:, :, 0]\n",
    "\n",
    "    # If it's still not 2D, try squeeze\n",
    "    if arr.ndim > 2:\n",
    "        arr = np.squeeze(arr)\n",
    "    if arr.ndim != 2:\n",
    "        # Fallback: flatten and reshape to (H, W) if possible, otherwise take first 2D slice\n",
    "        try:\n",
    "            arr = arr.reshape(arr.shape[0], -1)\n",
    "        except Exception:\n",
    "            arr = np.asarray(arr)\n",
    "            if arr.size:\n",
    "                arr = np.asarray(arr).ravel().reshape(1, -1)\n",
    "            else:\n",
    "                arr = np.zeros((1, 1), dtype=np.float32)\n",
    "\n",
    "    # Map values to [0,1]\n",
    "    if np.nanmax(arr) <= 1.0 and np.nanmin(arr) >= -1.0:\n",
    "        disp = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        amin, amax = np.nanmin(arr), np.nanmax(arr)\n",
    "        if amax > amin:\n",
    "            disp = (arr - amin) / (amax - amin)\n",
    "        else:\n",
    "            disp = np.clip(arr, 0.0, 1.0)\n",
    "\n",
    "    disp = np.nan_to_num(disp, nan=0.0)\n",
    "    disp = np.clip(disp, 0.0, 1.0)\n",
    "    return disp\n",
    "\n",
    "\n",
    "def load_npy_for_display(path):\n",
    "    arr = np.load(path)\n",
    "    return _to_hw(arr)\n",
    "\n",
    "\n",
    "def load_image_for_display(path):\n",
    "    img = Image.open(path).convert('L')\n",
    "    arr = np.asarray(img).astype(np.float32)\n",
    "    # Scale 0..255 -> 0..1\n",
    "    if arr.max() > 1.0:\n",
    "        arr = arr / 255.0\n",
    "    arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def visualize_quad_from_s1sr(\n",
    "    s1sr_dir,\n",
    "    s1t1_dir,\n",
    "    s1t2_dir,\n",
    "    s1hr_dir,\n",
    "    output_dir,\n",
    "    image_extensions=('*.png', '*.jpg', '*.jpeg', '*.tif', '*.tiff'),\n",
    "    max_images=None,\n",
    "    show=False,\n",
    "    verbose=True,\n",
    "    figsize=(14, 6),\n",
    "    cmap='gray'\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = []\n",
    "    for ext in image_extensions:\n",
    "        files.extend(glob.glob(os.path.join(s1sr_dir, ext)))\n",
    "    files.sort()\n",
    "\n",
    "    if max_images:\n",
    "        files = files[:max_images]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Found {len(files)} S1SR files in {s1sr_dir}\")\n",
    "\n",
    "    for s1sr_path in tqdm(files, desc=\"Visualizing quads\"):\n",
    "        basename = os.path.splitext(os.path.basename(s1sr_path))[0]\n",
    "\n",
    "        s1t1_path = find_matching_npy(basename, s1t1_dir)\n",
    "        s1t2_path = find_matching_npy(basename, s1t2_dir)\n",
    "        s1hr_path = find_matching_npy(basename, s1hr_dir)\n",
    "\n",
    "        if s1t1_path is None or s1t2_path is None or s1hr_path is None:\n",
    "            if verbose:\n",
    "                print(f\"Skipping {basename}: missing counterpart -> S1T1: {bool(s1t1_path)}, S1T2: {bool(s1t2_path)}, S1HR: {bool(s1hr_path)}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            a1 = load_npy_for_display(s1t1_path)\n",
    "            a2 = load_npy_for_display(s1t2_path)\n",
    "            a3 = load_image_for_display(s1sr_path)\n",
    "            a4 = load_npy_for_display(s1hr_path)\n",
    "\n",
    "            # Create gridspec: top row images (4), bottom row small histograms (for S1SR and S1HR)\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            gs = fig.add_gridspec(2, 4, height_ratios=[3, 1], hspace=0.25, wspace=0.05)\n",
    "\n",
    "            axes = [fig.add_subplot(gs[0, i]) for i in range(4)]\n",
    "            hist_s1sr_ax = fig.add_subplot(gs[1, 2])\n",
    "            hist_s1hr_ax = fig.add_subplot(gs[1, 3])\n",
    "\n",
    "            for ax, im, title in zip(axes, [a1, a2, a3, a4], ['S1T1', 'S1T2', 'S1SR', 'S1HR']):\n",
    "                ax.imshow(im, cmap=cmap, vmin=0, vmax=1)\n",
    "                ax.set_title(title, fontsize=10)\n",
    "                ax.axis('off')\n",
    "\n",
    "            # Histograms\n",
    "            s1sr_vals = a3.ravel()\n",
    "            s1hr_vals = a4.ravel()\n",
    "\n",
    "            # Avoid all-NaN\n",
    "            if np.isfinite(s1sr_vals).any():\n",
    "                hist_s1sr_ax.hist(s1sr_vals[np.isfinite(s1sr_vals)], bins=50, range=(0, 1), color='C0')\n",
    "                mu, sigma = np.nanmean(s1sr_vals), np.nanstd(s1sr_vals)\n",
    "                hist_s1sr_ax.set_title(f'S1SR hist μ={mu:.3f} σ={sigma:.3f}', fontsize=9)\n",
    "                hist_s1sr_ax.set_xlim(0, 1)\n",
    "            else:\n",
    "                hist_s1sr_ax.text(0.5, 0.5, 'No valid S1SR data', ha='center')\n",
    "                hist_s1sr_ax.set_xticks([])\n",
    "                hist_s1sr_ax.set_yticks([])\n",
    "\n",
    "            if np.isfinite(s1hr_vals).any():\n",
    "                hist_s1hr_ax.hist(s1hr_vals[np.isfinite(s1hr_vals)], bins=50, range=(0, 1), color='C1')\n",
    "                mu, sigma = np.nanmean(s1hr_vals), np.nanstd(s1hr_vals)\n",
    "                hist_s1hr_ax.set_title(f'S1HR hist μ={mu:.3f} σ={sigma:.3f}', fontsize=9)\n",
    "                hist_s1hr_ax.set_xlim(0, 1)\n",
    "            else:\n",
    "                hist_s1hr_ax.text(0.5, 0.5, 'No valid S1HR data', ha='center')\n",
    "                hist_s1hr_ax.set_xticks([])\n",
    "                hist_s1hr_ax.set_yticks([])\n",
    "\n",
    "            # Main title slightly raised and tight layout\n",
    "            fig.suptitle(basename, y=0.97, fontsize=11)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "            out_path = os.path.join(output_dir, basename + '.png')\n",
    "            plt.savefig(out_path, dpi=150, bbox_inches='tight', pad_inches=0.03)\n",
    "            if not show:\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error processing {basename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved visualizations to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c71d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total S1SR images: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "path_s1sr = \"/mnt/data1tb/vinh/ISSM-SAR/test_results_2\"\n",
    "path_s1t1 = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune_splited/val/S1T1\"\n",
    "path_s1t2 = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune_splited/val/S1T2\"\n",
    "path_s1hr = \"/mnt/data1tb/vinh/ISSM-SAR/dataset/fine-tune_splited/val/S1HR\"\n",
    "\n",
    "save_dir = \"/mnt/data1tb/vinh/ISSM-SAR/vis_results_probability_2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def to_2d(img):\n",
    "    \"\"\"\n",
    "    (1,H,W) -> (H,W)\n",
    "    \"\"\"\n",
    "    if img.ndim == 3 and img.shape[0] == 1:\n",
    "        return img[0]\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_png_norm(path):\n",
    "    \"\"\"\n",
    "    Load grayscale png [0,255] -> [-1,1]\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert(\"L\")\n",
    "    img = np.array(img).astype(np.float32)\n",
    "    img = img / 127.5 - 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "    \n",
    "def visualize_and_save(s1t1, s1t2, s1sr, s1hr, title, save_path):\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    gs = GridSpec(\n",
    "        2, 4,\n",
    "        height_ratios=[3, 1],\n",
    "        hspace=0.18,\n",
    "        wspace=0.05\n",
    "    )\n",
    "\n",
    "    # Chuẩn hoá về 2D để hiển thị\n",
    "    s1t1 = to_2d(s1t1)\n",
    "    s1t2 = to_2d(s1t2)\n",
    "    s1sr = to_2d(s1sr)\n",
    "    s1hr = to_2d(s1hr)\n",
    "\n",
    "    images = [s1t1, s1t2, s1sr, s1hr]\n",
    "    labels = [\"S1T1\", \"S1T2\", \"S1SR\", \"S1HR\"]\n",
    "\n",
    "    # --------------------\n",
    "    # Row 1: Images\n",
    "    # --------------------\n",
    "    for i, (img, label) in enumerate(zip(images, labels)):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ax.imshow(img, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # --------------------\n",
    "    # Chuẩn bị histogram\n",
    "    # --------------------\n",
    "    data_t = [\n",
    "        s1t1.flatten(),\n",
    "        s1t2.flatten(),\n",
    "    ]\n",
    "\n",
    "    # Tính max density chung CHỈ cho S1T1 & S1T2\n",
    "    max_density_t = 0\n",
    "    for x in data_t:\n",
    "        counts, _ = np.histogram(\n",
    "            x,\n",
    "            bins=100,\n",
    "            range=(-1, 1),\n",
    "            density=True\n",
    "        )\n",
    "        max_density_t = max(max_density_t, counts.max())\n",
    "\n",
    "    # --------------------\n",
    "    # Row 2: Histograms\n",
    "    # --------------------\n",
    "    data_all = [\n",
    "        s1t1.flatten(),\n",
    "        s1t2.flatten(),\n",
    "        s1sr.flatten(),\n",
    "        s1hr.flatten(),\n",
    "    ]\n",
    "\n",
    "    for i, (x, label) in enumerate(zip(data_all, labels)):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ax.hist(\n",
    "            x,\n",
    "            bins=100,\n",
    "            range=(-1, 1),\n",
    "            density=True\n",
    "        )\n",
    "        ax.set_title(f\"Histogram {label}\", fontsize=9)\n",
    "        ax.set_xlim(-1, 1)\n",
    "\n",
    "        # Chỉ ép Y cho S1T1 & S1T2\n",
    "        if i < 2:\n",
    "            ax.set_ylim(0, max_density_t)\n",
    "        else:\n",
    "            ax.autoscale(axis=\"y\")\n",
    "\n",
    "        # Gọn trục Y\n",
    "        # if i != 0:\n",
    "        #     ax.set_yticks([])\n",
    "\n",
    "    # --------------------\n",
    "    # Save\n",
    "    # --------------------\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "\n",
    "    plt.savefig(\n",
    "        save_path,\n",
    "        dpi=200,\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0.02\n",
    "    )\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "s1sr_list = sorted(glob.glob(os.path.join(path_s1sr, \"*.png\")))\n",
    "print(\"Total S1SR images:\", len(s1sr_list))\n",
    "\n",
    "for s1sr_path in s1sr_list:\n",
    "    name = os.path.splitext(os.path.basename(s1sr_path))[0]\n",
    "\n",
    "    s1t1_path = os.path.join(path_s1t1, name + \".npy\")\n",
    "    s1t2_path = os.path.join(path_s1t2, name + \".npy\")\n",
    "    s1hr_path = os.path.join(path_s1hr, name + \".npy\")\n",
    "\n",
    "    if not (os.path.exists(s1t1_path) and \n",
    "            os.path.exists(s1t2_path) and \n",
    "            os.path.exists(s1hr_path)):\n",
    "        print(f\"[SKIP] Missing pair: {name}\")\n",
    "        continue\n",
    "\n",
    "    s1t1 = np.load(s1t1_path)\n",
    "    s1t2 = np.load(s1t2_path)\n",
    "    s1hr = np.load(s1hr_path)\n",
    "    s1sr = load_png_norm(s1sr_path)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name + \".png\")\n",
    "\n",
    "    visualize_and_save(\n",
    "        s1t1=s1t1,\n",
    "        s1t2=s1t2,\n",
    "        s1sr=s1sr,\n",
    "        s1hr=s1hr,\n",
    "        title=name,\n",
    "        save_path=save_path\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "issm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
